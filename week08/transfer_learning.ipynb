{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b6100a-76fb-4368-a9e5-b598547392c5",
   "metadata": {},
   "source": [
    "Каждый раз, обучая нейронку, мы сначала рандомно инициализируем веса, а после в ходе бэкпропа обучаем модель. Если мы сразу же угадываем хорошие веса, модель сходится быстрее. Иногда можно брать в качестве инициализации веса, полученные другими исследователями и на их основе дообучать модель под свой выход. Это здорово упрощает задачу обучения и экономит недели работы.\n",
    "\n",
    "Transfer learning — это когда вы берёте чужую модель и адаптируете её под свою задачу. В этой тетрадке мы посмотрим на то, как в PyTorch можно это сделать.\n",
    "\n",
    "В прошлый раз мы обсуждали историческое развитие разных нейросетевых архитектур от AlexNet (2012 года) до ResNet (2015 года). Сегодня мы возьмём предобученный ResNet-18 из Torchvision и переделаем его так, чтобы он начал решать задачу классификации изображений на новом датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8096f45f-1167-4a61-9b37-52681c1ac9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "import PIL\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gWtv2YeYFCJW",
   "metadata": {},
   "source": [
    "# 1. Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3TXsROTKFUvR",
   "metadata": {},
   "source": [
    "Сегодня мы попробуем решить проблему тысячелетия вслед за [лучшими китайскими учёными](https://www.youtube.com/watch?v=vIci3C4JkL0).\n",
    "\n",
    "![](https://www.semantics3.com/blog/content/images/downloaded_images/hot-dog-and-a-not-hot-dog-the-distinction-matters-code-included-8550067fb16/1-VrpXE1hE4rO1roK0laOd7g.png)\n",
    "\n",
    "Мы будем отличать хот-доги от всего остального."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6M70WCGRHOBL",
   "metadata": {},
   "source": [
    "## 1.1. Скачиваем датасет\n",
    "\n",
    "Данные мы возьмём из [соревнования на Kaggle](https://www.kaggle.com/c/hotdogornot). Поскольку в Colab затруднительно скачать данные напрямую с Kaggle (для этого нужно добывать API-ключ), я залил обучающую выборку из этого соревнования на [Google Drive](https://drive.google.com/file/d/1IkDqUUidWfB0l_OnO239OZMVCUONiJUF/view?usp=sharing). Скачать её оттуда можно при помощи команды `gdown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657f264-37a7-4a5a-9241-795af5ede43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path('train_kaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xjpftlqHNlak",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data_root.exists():\n",
    "    !gdown https://drive.google.com/uc?id=1IkDqUUidWfB0l_OnO239OZMVCUONiJUF\n",
    "    !unzip -q kaggle_hotdogornot_train.zip\n",
    "    assert data_root.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e3636",
   "metadata": {},
   "source": [
    "## 1.2. Смотрим на датасет глазами\n",
    "\n",
    "В Colab есть примитивный браузер файлов, который позволяет посмотреть, что мы там такое скачали. Кроме того, мы можем повыполнять разные команды, чтобы получить представление о содержимом датасета.\n",
    "\n",
    "Консольные команды из ноутбука можно выполнять так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d81b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo 'this is a command'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ui4BaPBhH7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сколько файлов лежит в data_root?\n",
    "<YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gnTRtxkVIHq6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Как называются первые несколько файлов?\n",
    "<YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hhB4BfjYIQaX",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Верно ли, что все файлы имеют имена вида <класс>_<число>.jpg?\n",
    "import pandas as pd\n",
    "\n",
    "records = []\n",
    "for p in data_root.iterdir():\n",
    "    # p.suffix — расширение файла\n",
    "    # p.stem — имя без расширения\n",
    "    \n",
    "    <YOUR CODE>\n",
    "    \n",
    "    records.append({\n",
    "        'name': name,\n",
    "        'klass': klass,\n",
    "        'num': num,\n",
    "    })\n",
    "\n",
    "df_files = pd.DataFrame(records)\n",
    "df_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XH-NhbMpJNfi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Какие классы есть в датасете?\n",
    "df_files.klass.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mvzMRl8cJYnP",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Как выглядят случайные примеры?\n",
    "PIL.Image.open(data_root / df_files[df_files.klass == 'hotdog'].sample().name.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VdvrPXSbHrtS",
   "metadata": {},
   "source": [
    "## 1.3. Пишем обёртку `torch.utils.data.Dataset`\n",
    "\n",
    "А теперь напишем обёртку-наследника `torch.utils.data.Dataset`, чтобы дальше работать с этим датасетом. Нам понадобится следующее:\n",
    "\n",
    "* Распарсить все имена файлов и для каждого файла извлечь класс\n",
    "* Захардкодить или иным способом зафиксировать порядок классов в датасете (чтобы он не менялся между обучением и использованием модели)\n",
    "* Разделить датасет на обучающую и валидационную выборку со стратификацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a21f7c-c64f-42f4-9c55-ba0e04d8a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class HotDogOrNotDataset(torch.utils.data.Dataset):\n",
    "    classes = ['pets', 'furniture', 'people', 'food', 'frankfurter', 'chili-dog', 'hotdog']\n",
    "    \n",
    "    def __init__(self, data_root: Path, split='train', transform=None):\n",
    "        super().__init__()\n",
    "        assert split in {'train', 'test'}, f'Unknown split value: {split}'\n",
    "        self.data_root = data_root\n",
    "        self.split = split\n",
    "        self.transform = transform  # Преобразование, применяемое ко всем загружаемым элементам датасета\n",
    "        \n",
    "        self.class_to_idx = {klass: i for i, klass in enumerate(self.classes)}\n",
    "        \n",
    "        paths = sorted(data_root.iterdir(), key=lambda p: int(p.stem.split('_')[1]))\n",
    "        indices_train, indices_test = train_test_split(\n",
    "            range(len(paths)),\n",
    "            stratify=[self.class_to_idx[self._path_to_class(p)] for p in paths],\n",
    "            test_size=0.2, random_state=42)\n",
    "        \n",
    "        if split == 'train':\n",
    "            indices = set(indices_train)\n",
    "        else:\n",
    "            indices = set(indices_test)\n",
    "        \n",
    "        self.filenames = [p.name for i, p in enumerate(paths) if i in indices]\n",
    "        \n",
    "    @staticmethod\n",
    "    def _path_to_class(path: Path):\n",
    "        \"\"\"Given a path like {dataset_root}/{class}_{idx}.jpg, return class.\"\"\"\n",
    "        return path.stem.split('_')[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.data_root / self.filenames[idx]\n",
    "        \n",
    "        X = PIL.Image.open(path)\n",
    "        y = self.class_to_idx[self._path_to_class(path)]\n",
    "        \n",
    "        # Применяем преобразование, заданное при инициализации. Именно так работают аугментации.\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            'Dataset HotDogOrNot',\n",
    "            f'    Number of datapoints: {len(self)}',\n",
    "            f'    Root location: {self.data_root}',\n",
    "            f'    Split: {self.split}',\n",
    "        ])\n",
    "\n",
    "dataset_valid = HotDogOrNotDataset(data_root, split='test')\n",
    "dataset_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WdrwytvbLdc4",
   "metadata": {},
   "source": [
    "Проверим, что наш класс ведёт себя ожидаемым образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1336f656-35b1-43d7-8982-a23b3ea081ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dataset_valid[0]\n",
    "print(dataset_valid.classes[y])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bsj5Of0GMuVx",
   "metadata": {},
   "source": [
    "## 1.4. Преобразования валидационного датасета\n",
    "\n",
    "Теперь для примера попробуем реализовать какие-нибудь преобразования датасета. Поскольку это не самая сложная и важная задача, не будем акцентировать на этом большое внимание и реализуем только два преобразования:\n",
    "\n",
    "1. Изменение размеров изображения с сохранением пропорций;\n",
    "2. Вырезание куска из центра изображения.\n",
    "\n",
    "Здесь нужно сделать важное замечание. Мы целенаправленно используем для работы с изображениями библиотеку PIL (Python Imaging Library), потому что мы повторяем за torchvision, но, разумеется, нас никто не заставляет это делать. Мы могли бы пользоваться, например, библиотекой OpenCV и методами наподобие `cv2.imread` для загрузки изображений, возвращающими Numpy array, а не какие-то специализированные объекты типа `PIL.Image`. Альтернативная библиотека для аугментаций `albumentations` пошла именно по этому пути."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f26901-e092-4e12-bc92-d3e55f13b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(nn.Module):\n",
    "    def __init__(self, size: int):\n",
    "        super().__init__()\n",
    "        assert isinstance(size, int)\n",
    "        self.size = size\n",
    "    \n",
    "    def forward(self, image: PIL.Image):\n",
    "        # image is a PIL.Image, not a torch.Tensor, so we need to use PIL.Image methods\n",
    "        img_w, img_h = image.size\n",
    "        \n",
    "        # compute new_w and new_h\n",
    "        <YOUR CODE>\n",
    "        \n",
    "        return image.resize((new_w, new_h))\n",
    "\n",
    "assert Resize(256)(PIL.Image.new('RGB', (719, 960))).size in {(256, 342), (256, 341)}\n",
    "assert Resize(256)(PIL.Image.new('RGB', (960, 719))).size in {(342, 256), (341, 256)}\n",
    "\n",
    "Resize(256)(dataset_valid[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0090e8f7-6814-4a63-b411-4ebb208b62e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple\n",
    "\n",
    "class CenterCrop(nn.Module):\n",
    "    def __init__(self, crop_size: Union[int, Tuple[int, int]]):\n",
    "        super().__init__()\n",
    "        if isinstance(crop_size, tuple):\n",
    "            self.crop_h, self.crop_w = crop_size\n",
    "        else:\n",
    "            self.crop_h = self.crop_w = crop_size\n",
    "        \n",
    "    def forward(self, image: PIL.Image):\n",
    "        # image is a PIL.Image, not a torch.Tensor, so we need to use PIL.Image methods\n",
    "        img_w, img_h = image.size\n",
    "        \n",
    "        # compute left, top, right, bottom\n",
    "        # left & top will be included in the crop, right & bottom will be excluded\n",
    "        <YOUR CODE>\n",
    "        \n",
    "        assert left >= 0, left\n",
    "        assert top >= 0, top\n",
    "        assert right < img_w, (right, img_w)\n",
    "        assert bottom < img_h, (bottom, img_h)\n",
    "        \n",
    "        return image.crop((left, top, right, bottom))\n",
    "\n",
    "assert CenterCrop(224)(PIL.Image.new('RGB', (719, 960))).size == (224, 224)\n",
    "assert CenterCrop((224, 256))(PIL.Image.new('RGB', (719, 960))).size == (256, 224)\n",
    "\n",
    "CenterCrop(224)(dataset_valid[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GVmxQT_qPkOM",
   "metadata": {},
   "source": [
    "Ещё сделаем простейший класс-обёртку наподобие `Sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q75Ec7yqPjiw",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Compose(nn.Module):\n",
    "    def __init__(self, submodules: List[nn.Module]):\n",
    "        super().__init__()\n",
    "        self.submodules = nn.ModuleList(submodules)\n",
    "\n",
    "    def forward(self, image):\n",
    "        <YOUR CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96Uka4XMQJwl",
   "metadata": {},
   "source": [
    "Наконец, посмотрим на всю конструкцию в действии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b157c190-d7d6-4331-b211-f96a40ef8586",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_valid = HotDogOrNotDataset(\n",
    "    data_root, transform=Compose([\n",
    "        Resize(256),\n",
    "        CenterCrop(224),\n",
    "    ]),\n",
    "    split='test',\n",
    ")\n",
    "dataset_valid[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JzSHm4PKQYXY",
   "metadata": {},
   "source": [
    "На этом мы заканчиваем ручную реализацию трансформаций и переходим на стандартные трансформации из `torchvision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HEXjb0bCRxXz",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "dataset_valid = HotDogOrNotDataset(\n",
    "    data_root, transform=transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "    ]),\n",
    "    split='test',\n",
    ")\n",
    "dataset_valid[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eC6s6S3HRwGb",
   "metadata": {},
   "source": [
    "Для валидационной выборки, помимо `Resize`, `CenterCrop` и `Compose`, нам понадобятся ещё `ToTensor` и `Normalize`. `ToTensor` конвертирует `PIL.Image` в `torch.Tensor` и приблизительно эквивалентна следующему коду:\n",
    "\n",
    "```python\n",
    "def to_tensor(image):\n",
    "    return torch.tensor(np.array(img) / 255.).permute((2, 0, 1))\n",
    "```\n",
    "\n",
    "`Normalize` вычитает из изображения фиксированный `mean` и делит на фиксированный `std`. Он нужен из-за того, что ResNet-18, который мы будем использовать, был обучен с применением такой нормализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dad762-aa7c-4246-bc8b-59c5e6c898aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "dataset_valid = HotDogOrNotDataset(data_root, transform=transform_valid, split='test')\n",
    "\n",
    "dataset_valid[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w3Wk_r6JShrq",
   "metadata": {},
   "source": [
    "## 1.5. Преобразования обучающего датасета\n",
    "\n",
    "В обучении мы будем использовать простейшие аугментации:\n",
    "\n",
    "* `RandomResizedCrop`: вырезать случайный прямоугольник из изображения, после чего привести его к фиксированному размеру;\n",
    "* `RandomHorizontalFlip`: с вероятностью 0.5 отразить изображение по горизонтали.\n",
    "\n",
    "Давайте посмотрим на эффект от этих аугментаций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d32fb5d-91ff-4bbf-8456-673eb6923aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = HotDogOrNotDataset(\n",
    "    data_root, transform=transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ]),\n",
    "    split='train',\n",
    ")\n",
    "X, y = dataset_train[0]\n",
    "print(dataset_train.classes[y])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pw0lj09RUXMK",
   "metadata": {},
   "source": [
    "Вернём `ToTensor` и `Normalize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PtfJj74TUMmk",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "dataset_train = HotDogOrNotDataset(data_root, transform=transform_train, split='train')\n",
    "\n",
    "dataset_train[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8nEAlU_rSFgR",
   "metadata": {},
   "source": [
    "## 1.6. Даталоадеры\n",
    "\n",
    "Как обычно, заведём даталоадеры. Здесь я задал валидационному даталоадеру `shuffle=True`, чтобы потом была возможность посмотреть на случайный сэмпл предсказаний модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e5562e-e1cd-4ad5-8a6e-9ee9f15d1701",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 2\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "dataloader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43587ecc-7e49-4d65-959d-246bab549f9c",
   "metadata": {},
   "source": [
    "# 2. Реквизируем ResNet-18 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "szr7CKWOfiwg",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fe263-66bd-40d9-b9df-46cbae989561",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6qqWiE-0WTTh",
   "metadata": {},
   "source": [
    "## 2.1. Скачиваем предобученную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a71160-8bb5-4704-b02f-71f007f4be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True).to(device)\n",
    "\n",
    "# Не забываем про .eval(), чтобы отключить батчнормы\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18cc170-e6ac-4574-8d5c-596b53d88b48",
   "metadata": {},
   "source": [
    "## 2.2. Смотрим, на что она способна\n",
    "\n",
    "Теперь попробуем что-нибудь спрогнозировать. Вспомним, как мы делали это с VGG-16 на самой первой неделе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a8619-c08b-463c-88df-f77ba9c109f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(url):\n",
    "    response = requests.get(url)\n",
    "    img = PIL.Image.open(io.BytesIO(response.content))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f06e6-eb72-49cf-8baf-28c0ddc35641",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_URL = 'https://upload.wikimedia.org/wikipedia/en/5/5f/Original_Doge_meme.jpg'\n",
    "# IMG_URL = 'https://sadanduseless.b-cdn.net/wp-content/uploads/2019/06/cat-breading4.jpg'\n",
    "# IMG_URL = 'https://images-na.ssl-images-amazon.com/images/I/91NKh-FPcBL._SL1500_.jpg'\n",
    "# IMG_URL = 'https://sun9-34.userapi.com/c850216/v850216669/110118/s1XSv_XLgtY.jpg'\n",
    "\n",
    "image = get_image(IMG_URL)\n",
    "print(f'Image size: {image.size}')\n",
    "image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-0SlXJ-VWzTG",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_URL = 'https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt'\n",
    "\n",
    "response = requests.get(LABELS_URL)\n",
    "labels = np.array(response.content.decode('utf-8').split('\\n'))\n",
    "\n",
    "print(f'Total labels: {len(labels)}')\n",
    "print(f'Example labels: {labels[200:205]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n2TADCnRbMeA",
   "metadata": {},
   "source": [
    "Завернём весь процесс предсказания в функцию. Заодно познакомимся с функциональным интерфейсом к `torchvision.transforms`, чтобы можно было видеть, что именно получает на вход нейросеть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ya4BPl28YIkT",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import torchvision.transforms.functional\n",
    "\n",
    "def predict(image):\n",
    "    image = transforms.functional.resize(image, 256)\n",
    "    image = transforms.functional.center_crop(image, 224)\n",
    "    \n",
    "    # Показываем картинку после resize и crop, но до преобразования в тензор:\n",
    "    display(image)\n",
    "    \n",
    "    tensor = transforms.functional.to_tensor(image)\n",
    "    tensor = transforms.functional.normalize(tensor, mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "    \n",
    "    # Добавьте размерность батча\n",
    "    tensor = <YOUR CODE>\n",
    "    # Перенесите tensor на GPU\n",
    "    tensor = <YOUR CODE>\n",
    "    \n",
    "    # Предскажите logits_tensor, предварительно отключив сохранение вычислительного графа:\n",
    "    <YOUR CODE>\n",
    "    \n",
    "    # Уберите размерность батча\n",
    "    logits_tensor = <YOUR CODE>\n",
    "\n",
    "    # Здесь можно было бы сразу посчитать и вернуть argmax,\n",
    "    # если бы нас интересовало только одно предсказание.\n",
    "    # Но мы посчитаем top-5 предсказаний. В прошлый раз мы\n",
    "    # это делали таким кодом на Numpy:\n",
    "    # \n",
    "    # logits = logits.cpu().numpy()\n",
    "    # indices = logits.argsort()[-5:][::-1]\n",
    "    # probs = scipy.special.softmax(logits)\n",
    "    # \n",
    "    # Сейчас мы воспользуемся уже известной нам torch.nn.functional.softmax(),\n",
    "    # а также полезной функцией torch.topk(), имеющей такой интерфейс:\n",
    "    # \n",
    "    # >>> torch.topk(x, 5)\n",
    "    # torch.return_types.topk(values=tensor([13.2613, 12.7950, 12.5249, 12.4262, 11.8144]), indices=tensor([260, 259, 273, 263, 151]))\n",
    "\n",
    "    probs = <YOUR CODE>\n",
    "    topk = <YOUR CODE>\n",
    "    \n",
    "    for idx in topk.indices:\n",
    "        print(f'{probs[idx] * 100:>5.2f}% | {labels[idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dayAfiCaYjAh",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IsOAck-adFI7",
   "metadata": {},
   "source": [
    "Напоследок посмотрим, как выглядят прогнозы на нашем датасете для хот-догов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XqzqqZUadMN6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch, y_batch = next(iter(dataloader_valid))\n",
    "X_batch = X_batch.to(device)\n",
    "y_batch = y_batch.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_tensor = model(X_batch)\n",
    "\n",
    "probs_tensor = F.softmax(logits_tensor, dim=-1)\n",
    "probs = probs_tensor.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beabb68-7d00-448d-b34e-ac1e1320100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 5\n",
    "rows = 3\n",
    "fig, axarr = plt.subplots(rows, cols, figsize=(4 * cols - 1, 5 * rows - 1))\n",
    "\n",
    "k = 0 \n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        ax = axarr[i, j]\n",
    "        ax.grid('off')\n",
    "        ax.axis('off')\n",
    "        ax.imshow(np.clip(X_batch[k].permute(1, 2, 0).cpu().numpy() * IMAGENET_STD + IMAGENET_MEAN, 0, 1))\n",
    "        class_idx = probs[k].argmax()\n",
    "        ax.set_title(f'{probs[k, class_idx]:>6.2%} : {labels[class_idx]}', size=14)\n",
    "        k += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a2126c-8a52-4fb5-9f1d-bfa3cc02b15e",
   "metadata": {},
   "source": [
    "Модель отрабатывает на уровне выше всех похвал (но это неточно)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a28df3-6659-4863-bcdd-6b249c88d685",
   "metadata": {},
   "source": [
    "# 3. Хирургическое вмешательство\n",
    "\n",
    "Побаловавшись с прогнозами, займёмся более серьёзными проблемами.\n",
    "\n",
    "Предобученная сетка не приспособлена для работы с нашими классами. Давайте заставим её их выучить. Для этого нам придётся срезать с сетки её последние слои. Посмотрим на модель повнимательнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HFAIlVnBeztA",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae97b619-cc8c-427d-b9dc-79369d259f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "def print_summary(model):\n",
    "    summary(model, (3, 224, 224), device=torch.device(device).type)\n",
    "\n",
    "print_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7OqHxSbie7sS",
   "metadata": {},
   "source": [
    "## 3.1. Вырезаем feature extractor\n",
    "\n",
    "В прошлый раз, реализовывая эту модель, мы увидели, что она по сути является одной длинной последовательностью блоков, хотя почему-то в `torchvision` она не реализована как `nn.Sequential`.\n",
    "\n",
    "Наша задача — переиспользовать как можно больше слоёв из этой модели. В данном случае мы можем переиспользовать вообще всё, кроме последнего линейного слоя, по сути делающего логистическую регрессию поверх свёрточных фичей.\n",
    "\n",
    "Давайте создадим новую модель, которая будет возвращать эти самые свёрточные фичи. Для этого мы можем просто влезть внутрь предобученной модели, вытащить оттуда слои, и сделать из них новый `Sequential`. Единственный нюанс — после последнего усреднения получается тензор с шейпом `(B, 512, 1, 1)`, поэтому в конце надо сделать либо `Flatten`, либо несколько `squeeze()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HifnXpGUggRs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример того, как можно влезть внутрь модели:\n",
    "model.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe4dcff-e6fd-4290-a51c-e4d7d303b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрите внимательно на напечатанную выше структуру модели,\n",
    "# найдите все слои, которые там используются, и составьте из них\n",
    "# один nn.Sequential. В нём должно получиться примерно 10 слоёв.\n",
    "\n",
    "model_beheaded = <YOUR CODE>\n",
    "\n",
    "assert model_beheaded(X_batch).shape == (batch_size, 512)\n",
    "\n",
    "print_summary(model_beheaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809xnU9phLOX",
   "metadata": {},
   "source": [
    "## 3.2. Делаем feature extractor необучаемым\n",
    "\n",
    "Дальше мы приделаем к этой части новый линейный слой, который обучим делать логистическую регрессию на новом датасете. Но сейчас нам нужно сделать так, чтобы эта часть не обучалась. Для этого нужно отключить опцию `requires_grad` у всех параметров внутри неё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed8a23-6bf9-447e-b1a1-d1423b1b034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_params_requires_grad(model):\n",
    "    for name, p in model.named_parameters():\n",
    "        print(f'{name:<30} {str(p.shape):<30} {p.requires_grad}')\n",
    "\n",
    "print_params_requires_grad(model_beheaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf1cd30-51d2-4bef-821f-2496b7d57b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_beheaded(X_batch).requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o9pYAelliaC5",
   "metadata": {},
   "source": [
    "Делается это очень просто:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0dc28-3a1a-4129-94d4-bc600de334af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model_beheaded.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5142d8a7-3f45-43ce-a99a-ef6afa8b0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_params_requires_grad(model_beheaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a64ce0-508f-44a8-8ac4-d72ac6729c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_beheaded(X_batch).requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bi93xic4idxT",
   "metadata": {},
   "source": [
    "## 3.3. Создаём новый классификатор поверх старого feature extractor\n",
    "\n",
    "Теперь можно собрать новую модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa85dba-b340-4bee-90f5-11d82897d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_classifier = <YOUR CODE>\n",
    "# Не забудьте перенести новую голову на GPU!\n",
    "\n",
    "model_hotdog = <YOUR CODE>\n",
    "\n",
    "print_summary(model_hotdog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84453840-9e7b-4546-94e6-79c50f76e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_params_requires_grad(model_hotdog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a177f8-7795-4eaa-9b77-2be82a41a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_hotdog(X_batch)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7dc0d2-54a5-4358-839d-ce51bf7cc7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qJG6P3UMTwOm",
   "metadata": {},
   "source": [
    "# 4. Tensorboard\n",
    "\n",
    "Прежде чем это учить, познакомимся ещё с одним очень важным инструментом, полезным, когда вы ставите много долгих экспериментов, — Tensorboard. Это такая модная штука для визуализации логов.\n",
    "\n",
    "Работает она так. Перед началом обучения вы создаёте объект класса `SummaryWriter`, который будет писать логи в специальную папку. Параллельно вы запустите процесс `tensorboard`, который будет читать эту папку и визуализировать в веб-интерфейсе то, что там найдёт.\n",
    "\n",
    "По-хорошему, Tensorboard запускается из командной строки командой наподобие\n",
    "\n",
    "```bash\n",
    "tensorboard --port 6006 --logdir tb_logs\n",
    "```\n",
    "\n",
    "Но поскольку мы работаем в Colab, для нас всё будет устроено несколько иначе. Полную документацию можно посмотреть [тут](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_in_notebooks.ipynb), но если вкратце, то, во-первых, нужно подгрузить Jupyter extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ejNIMoAjTC-W",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fV3Lvs3oj46y",
   "metadata": {},
   "source": [
    "А потом запустить Tensorboard при помощи magic-команды, передав ей `--logdir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0crx14SqTFom",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir tb_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ULYbBKbxj-v_",
   "metadata": {},
   "source": [
    "Использовать `SummaryWriter` можно примерно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6AWPo3wqmk-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# На случай перезапуска следующей ячейки удалим записанные логи\n",
    "if Path('tb_logs/demo').exists():\n",
    "    !rm -r tb_logs/demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5R4IZrqQTKVA",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "with SummaryWriter(log_dir='tb_logs/demo') as writer:\n",
    "    for t in range(100):\n",
    "        writer.add_scalar('some_tag', np.sin(t / 20), t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Yz3LQepVkDp7",
   "metadata": {},
   "source": [
    "# 5. Дообучение\n",
    "\n",
    "А теперь давайте напишем функцию для обучения модели, но логи будем писать в Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b9e5b3-4d79-429f-b7fe-bfc50f0c631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, opt, dataloader_train, dataloader_valid, num_epochs, run_name, device='cuda:0'):\n",
    "    with SummaryWriter(log_dir=str(Path('tb_logs') / run_name)) as writer:\n",
    "        train_batches = 0\n",
    "        with tqdm(range(1, num_epochs + 1)) as epochs_progress_bar:\n",
    "            for epoch in epochs_progress_bar:\n",
    "                # Трейн\n",
    "                model.train()\n",
    "                with tqdm(dataloader_train, desc=f'Train | Epoch {epoch}') as train_progress_bar:\n",
    "                    for x_batch, y_batch in train_progress_bar:\n",
    "                        # Переносим батч на GPU\n",
    "                        x_batch = x_batch.to(device)\n",
    "                        y_batch = y_batch.to(device)\n",
    "\n",
    "                        y_pred = model(x_batch)  # делаем предсказания\n",
    "                        loss = criterion(y_pred, y_batch)  # считаем лосс\n",
    "\n",
    "                        loss_val = loss.item()\n",
    "                        writer.add_scalar('train/loss', loss_val, train_batches)\n",
    "                        assert np.isfinite(loss_val)\n",
    "\n",
    "                        # Считаем градиенты и делаем шаг оптимизатора, не забыв обнулить градиенты\n",
    "                        opt.zero_grad()\n",
    "                        loss.backward()\n",
    "                        opt.step()\n",
    "\n",
    "                        train_batches += 1\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    epoch_losses_valid = []\n",
    "                    epoch_correct_predictions_valid = []\n",
    "                    with tqdm(dataloader_valid, desc=f'Valid | Epoch {epoch}') as valid_progress_bar:\n",
    "                        for x_batch, y_batch in valid_progress_bar:\n",
    "                            # Переносим батч на GPU\n",
    "                            x_batch = x_batch.to(device)\n",
    "                            y_batch = y_batch.to(device)\n",
    "\n",
    "                            y_pred = model(x_batch)  # делаем предсказания\n",
    "                            loss = criterion(y_pred, y_batch)  # считаем лосс\n",
    "                            \n",
    "                            loss_val = loss.item()\n",
    "                            assert np.isfinite(loss_val)\n",
    "                            epoch_losses_valid.append(loss_val)\n",
    "\n",
    "                            batch_correct_predictions = torch.argmax(y_pred, dim=-1) == y_batch\n",
    "                            epoch_correct_predictions_valid.extend(batch_correct_predictions.to('cpu').numpy().tolist())\n",
    "\n",
    "                    writer.add_scalar('valid/loss', np.mean(epoch_losses_valid), epoch)\n",
    "                    writer.add_scalar('valid/accuracy', np.mean(epoch_correct_predictions_valid), epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "msWWNV09kMIH",
   "metadata": {},
   "source": [
    "Как обычно, создадим `criterion`, `opt`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HS0KyEPCbxBm",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model_hotdog.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D_3gjH-qkPv1",
   "metadata": {},
   "source": [
    "Полезный костыль: при записи логов Tensorboard в названии папки указывать текущее время. Код для этого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pcdo1cyOcOyy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_datetime():\n",
    "    return datetime.datetime.now().isoformat(sep='_', timespec='milliseconds').replace(':', '-')\n",
    "\n",
    "get_datetime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Te9QT2WJkZlr",
   "metadata": {},
   "source": [
    "Запускаем дообучение!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irkVPittcNsy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    model_hotdog, criterion, opt, dataloader_train, dataloader_valid,\n",
    "    num_epochs=5, run_name=f'{get_datetime()}_finetune-resnet18-5-epochs', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jdoHesEzko7f",
   "metadata": {},
   "source": [
    "# 5. Смотрим на результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mx7_Fbo9ktOW",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch, y_batch = next(iter(dataloader_valid))\n",
    "X_batch = X_batch.to(device)\n",
    "y_batch = y_batch.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_tensor = model_hotdog(X_batch)\n",
    "\n",
    "probs_tensor = F.softmax(logits_tensor, dim=-1)\n",
    "probs = probs_tensor.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nNAFWdVDhOPq",
   "metadata": {},
   "outputs": [],
   "source": [
    "[dataset_valid.classes[idx] for idx in probs.argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8-H-SD0hOPq",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 5\n",
    "rows = 3\n",
    "fig, axarr = plt.subplots(rows, cols, figsize=(4 * cols - 1, 5 * rows - 1))\n",
    "\n",
    "k = 0 \n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        ax = axarr[i, j]\n",
    "        ax.grid('off')\n",
    "        ax.axis('off')\n",
    "        ax.imshow(np.clip(X_batch[k].permute(1, 2, 0).cpu().numpy() * IMAGENET_STD + IMAGENET_MEAN, 0, 1))\n",
    "        class_idx = probs[k].argmax()\n",
    "        ax.set_title(f'{probs[k, class_idx]:>6.2%} : {dataset_valid.classes[class_idx]}', size=14)\n",
    "        k += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SziWttb_mVSg",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
