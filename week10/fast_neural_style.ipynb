{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2wq1pk5UVQzZ",
   "metadata": {},
   "source": [
    "# Быстрый style transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f21810c",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/2166/1*8bbp3loQjkLXaIm_QBfD8w.jpeg)\n",
    "\n",
    "В этом ноутбуке мы реализуем алгоритм для переноса стиля.\n",
    "\n",
    "[Классический](https://arxiv.org/abs/1508.06576) алгоритм для style transfer работает медленно: для того чтобы его применить к одному изображению, нужно прогнать целую процедуру оптимизации. Идея для его ускорения тривиальна, как всё в deep learning: предсказывать результат этой процедуры оптимизации отдельной нейросетью.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/37034031/42068027-830719f4-7b84-11e8-9e87-088f1e476aab.png)\n",
    "\n",
    "Эта идея была предложена в работе 2016 года под названием [\"Perceptual Losses for Real-Time Style Transfer and Super-Resolution\"](https://arxiv.org/abs/1603.08155). Код для ноутбука основан на примере [`fast_neural_style`](https://github.com/pytorch/examples/tree/master/fast_neural_style) из репозитория `pytorch/examples`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74351481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "import os\n",
    "import datetime\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VjJTyk5qPsCz",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d7749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_image_to_numpy(image_torch):\n",
    "    \"\"\"Convert PyTorch tensor to Numpy array.\n",
    "    :param image_torch: [0..1]-normalized PyTorch float CHW Tensor.\n",
    "    :returns: Numpy uint8 HWC array in range [0..255].\n",
    "    \"\"\"\n",
    "    assert len(image_torch.shape) == 3, 'Have you forgotten to remove the batch dimension?'\n",
    "    image_np = image_torch.permute(1, 2, 0).numpy()\n",
    "    image_np = image_np * 255 + 0.5\n",
    "    image_np = np.clip(image_np, 0, 255)\n",
    "    image_np = image_np.astype(np.uint8)\n",
    "    return image_np\n",
    "\n",
    "def get_datetime():\n",
    "    return datetime.datetime.now().isoformat(sep='_', timespec='milliseconds').replace(':', '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c26567",
   "metadata": {},
   "source": [
    "## Датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_8pNCNmEA5ef",
   "metadata": {},
   "source": [
    "Для обучения генератора картинок нам понадобится датасет. Подойдёт более-менее любой датасет, в котором распределение картинок похоже на то, на котором мы потом будем применять сетку. Здесь я предлагаю использовать датасет [Flickr8k](http://hockenmaier.cs.illinois.edu/8k-pictures.html), который выложен на [Kaggle](https://www.kaggle.com/adityajn105/flickr8k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4352127",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path('flickr8k')\n",
    "\n",
    "if not data_root.exists():\n",
    "    !gdown https://drive.google.com/uc?id=1DEEYahajtFjxkWXdRHp5hXF1H952tAD2\n",
    "    !unzip -q flickr8k.zip -d $data_root\n",
    "    assert data_root.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nc5yOQssCJkm",
   "metadata": {},
   "source": [
    "Датасет вообще-то предназначается для image captioning, то есть он содержит пары из картинок и текста, описывающего эту картинку. Текст мы использовать не будем.\n",
    "\n",
    "Давайте посмотрим на этот датасет глазами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yCT01TOLCboF",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l $data_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3-9Dz1WLChjJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {data_root}/Images | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fb_znM-BCzwm",
   "metadata": {},
   "source": [
    "А вот так выглядят аннотации, которые мы не будем использовать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RYJnFo_SCrDA",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {data_root}/captions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCX5Nqx_DBhh",
   "metadata": {},
   "source": [
    "В Torchvision есть удобный класс `ImageFolder`, который позволяет загружать разного рода картиночные датасеты. В нашем случае можно его использовать примерно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iwEZkmSrDNYM",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(data_root)\n",
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cwx7FX2Dkqo",
   "metadata": {},
   "source": [
    "Но нам понадобится, как обычно, сделать некоторую предобработку этих картинок, прежде чем мы сможем передать их на вход в нейросеть. Давайте в этот раз сделаем такую предобработку:\n",
    "\n",
    "* Масштабирование так, чтобы меньшая сторона изображения стала длиной 256 пикселей, с сохранением соотношения сторон;\n",
    "* Вырезание квадрата из центра изображения;\n",
    "* Преобразование `PIL.Image` в `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e6b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    <YOUR CODE>\n",
    "    # Почему мы не делаем .to(device) прямо в transform:\n",
    "    # https://discuss.pytorch.org/t/to-device-gives-an-error-when-used-inside-transforms-compose/51387\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(data_root, transform=transform)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dGJQG_XlEEtf",
   "metadata": {},
   "source": [
    "Как обычно, заведём dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd27a1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a1bc1d",
   "metadata": {},
   "source": [
    "## Картинка со стилем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba3bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(io.BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac51d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_url = 'https://github.com/pytorch/examples/raw/master/fast_neural_style/images/style-images/mosaic.jpg'\n",
    "style_image = get_image(style_url)\n",
    "style_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51681f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_tensor = transform(style_image)\n",
    "Image.fromarray(torch_image_to_numpy(style_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-SKqaW9TQe73",
   "metadata": {},
   "source": [
    "## Картинка для валидации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19NtzqIfPnLL",
   "metadata": {},
   "source": [
    "Во время обучения сетки-генератора мы будем периодически проверять текущее качество на этой картинке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_url = 'https://github.com/pytorch/examples/raw/master/fast_neural_style/images/content-images/amber.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b7a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = get_image(content_url)\n",
    "content_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062e6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_tensor = transform(content_image)\n",
    "Image.fromarray(torch_image_to_numpy(content_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3d4ea5",
   "metadata": {},
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ut5KT3hqnxaQ",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JQcV2pQzP2OI",
   "metadata": {},
   "source": [
    "Для обучения понадобится прогонять то, что сгенерировал генератор, через сеть VGG-16, и извлекать из неё промежуточные представления. Вспомним, что такое VGG-16:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fTSkiz2AQLJn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сразу укажем pretrained=True, чтобы веса скачались заранее\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RJtRJF2CQb_7",
   "metadata": {},
   "source": [
    "На семинаре мы будем использовать промежуточные представления, которые получаются перед **первыми 4 макс-пулингами**, сразу после ReLU.\n",
    "\n",
    "Это не означает, что нельзя использовать другие. Вы можете поэкспериментировать с любыми представлениями!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NTCl7ZYxRCkq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Какие у этих ReLU индексы?\n",
    "layer_indices = [3, 8, 15, 22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c0127",
   "metadata": {},
   "outputs": [],
   "source": [
    "VggOutputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3'])\n",
    "\n",
    "\n",
    "class Vgg16(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Для удобства загоним прямо внутрь нашего класса imagenet-нормализацию,\n",
    "        # чтобы потом о ней не думать\n",
    "        self.register_buffer('imagenet_mean', torch.tensor([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1))\n",
    "        self.register_buffer('imagenet_std', torch.tensor([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1))\n",
    "        \n",
    "        # Приготовьтесь к тому, чтобы в forward() выдавать значения промежуточных слоёв\n",
    "        <YOUR CODE>\n",
    "\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = (X - self.imagenet_mean) / self.imagenet_std\n",
    "\n",
    "        # Вычислите активации на 4 выбранных слоях и дайте им названия, как у аргументов VggOutputs\n",
    "        <YOUR CODE>\n",
    "        \n",
    "        out = VggOutputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3)\n",
    "        return out\n",
    "\n",
    "\n",
    "vgg = Vgg16()\n",
    "vgg = vgg.to(device)\n",
    "features = vgg(torch.randn(1, 3, 224, 224).to(device))\n",
    "features.relu4_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c2f1b9",
   "metadata": {},
   "source": [
    "## Генератор"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KgHF-kSpTLDU",
   "metadata": {},
   "source": [
    "Перейдём к самой интересной части — image-to-image сетке, генерирующей стилизованные изображения. В качестве базового кирпичика в ней мы будем использовать последовательность из свёртки, нормализации и активации. Также опционально этот кирпичик будет делать увеличивать картинку перед свёрткой, что понадобится нам во второй половине генератора.\n",
    "\n",
    "В качестве нормализации здесь мы будем использовать не batch normalization, а так называемый instance normalization. Сравните:\n",
    "\n",
    "Batch normalization:\n",
    "\n",
    "```python\n",
    "    x - x.mean(dim=(0, 2, 3))\n",
    "y = -------------------------\n",
    "      x.var(dim=(0, 2, 3))\n",
    "```\n",
    "\n",
    "Instance normalization:\n",
    "\n",
    "```python\n",
    "    x - x.mean(dim=(2, 3))\n",
    "y = ----------------------\n",
    "      x.var(dim=(2, 3))\n",
    "```\n",
    "\n",
    "(Разумеется, это не точная формулировка обеих нормализаций: здесь опущены нюансы про biased/unbiased variance estimation, никак не упоминается скользящее среднее, нет ничего про аффинное преобразование после нормализации, отсутствует ε в знаменателе и так далее. Цель здесь — это показать разницу между двумя нормализациями. Технические детали можете посмотреть тут: [batch norm](https://github.com/dniku/dl-norms/blob/master/dl_norms/batch_norm.py), [instance norm](https://github.com/dniku/dl-norms/blob/master/dl_norms/instance_norm.py).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_norm_act(in_channels, out_channels, kernel_size, stride=1, upsample : Optional[int] = None, norm=True, relu=True):\n",
    "    layers = []\n",
    "    if upsample is not None:\n",
    "        # An upsample followed by a convolution gives better results compared to ConvTranspose2d.\n",
    "        # ref: http://distill.pub/2016/deconv-checkerboard/\n",
    "        layers.append(nn.Upsample(mode='nearest', scale_factor=upsample))\n",
    "    layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=kernel_size // 2))\n",
    "    if norm:\n",
    "        layers.append(nn.InstanceNorm2d(out_channels, affine=True))\n",
    "    if relu:\n",
    "        layers.append(nn.ReLU())\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pX4CGwQsXwba",
   "metadata": {},
   "source": [
    "Кирпичик мы будем использовать как по отдельности, так и в составе residual-блока, как в резнете. Давайте опишем такой блок.\n",
    "\n",
    "```\n",
    "----> [conv(3x3)->norm->relu] --> [conv(3x3)->norm] --> + -->\n",
    "  |                                                     ↑\n",
    "  |_____________________________________________________|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RBRpBS1wTizZ",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        <YOUR CODE>\n",
    "\n",
    "    def forward(self, x):\n",
    "        <YOUR CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qgXbieizYyoK",
   "metadata": {},
   "source": [
    "Наконец, опишем сам генератор. Дадим ему такую структуру:\n",
    "\n",
    "```\n",
    "Вход: картинка с 3 каналами\n",
    "\n",
    "image ->\n",
    "[conv(32,9x9)->norm->relu] ->\n",
    "[conv(64,3x3,stride=2)->norm->relu] ->\n",
    "    [conv(128,3x3,stride=2)->norm->relu] ->\n",
    "        (5 раз) ResidualBlock ->\n",
    "        [upsample(x2)->conv(64,3x3)->norm->relu] ->\n",
    "    [upsample(x2)->conv(32,3x3)->norm->relu] ->\n",
    "[upsample(x2)->conv(3,9x9)] ->\n",
    "sigmoid ->\n",
    "stylized image\n",
    "```\n",
    "\n",
    "Вопрос: как определяются отступы строк в этой схеме?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gbTFOWaEYt6Z",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = <YOUR CODE>\n",
    "transformer = transformer.to(device)\n",
    "transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bhc3Y2k7aWVu",
   "metadata": {},
   "source": [
    "Посмотрим, что необученный генератор выдаёт на первой картинке из датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.hstack([\n",
    "    torch_image_to_numpy(dataset[0][0]),\n",
    "    torch_image_to_numpy(transformer(dataset[0][0].unsqueeze(0).to(device)).squeeze(0).detach().cpu()),\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35090c02",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b16e818",
   "metadata": {},
   "source": [
    "### Content loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6UiTqt6PanGk",
   "metadata": {},
   "source": [
    "Лосс в style transfer состоит из двух частей. Первая из них — content loss, отвечающий за то, чтобы сохранять семантику картинки. Также его называют feature reconstruction loss или perceptual loss. Формула для него такая:\n",
    "\n",
    "$$\n",
    "l_{\\text{content}}(\\hat y, y) = \\frac 1 {B C_j H_j W_j} || \\text{VGG}_j(\\hat y) - \\text{VGG}_j (y) ||_2^2\n",
    "$$\n",
    "\n",
    "Здесь $\\hat y$ — батч сгенерированных изображений, $y$ — батч из content images, VGG подразумевает уже подготовленную нами VGG-16, а $j$ — это индекс слоя в ней. Мы будем использовать $j = \\text{relu2_2}$. $B$, $C_j$, $H_j$ и $W_j$ — это размеры соответствующего тензора активаций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lKrKoO1TR2HT",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_loss(gen_features, content_features):\n",
    "    <YOUR CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b720b632",
   "metadata": {},
   "source": [
    "### Style loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1Nxh1v8wcpFR",
   "metadata": {},
   "source": [
    "Вторая компонента лосса — это style loss (или style reconstruction loss). Он записывается немного сложнее:\n",
    "\n",
    "$$\n",
    "l_{\\text{style}}(\\hat y, y) = \\sum_{j} \\frac 1 {B C_j^2} \\left|\\left| \\frac 1 {C_j H_j W_j} \\text{Gram}_j(\\hat y) - \\frac 1 {C_j H_j W_j} \\text{Gram}_j (y) \\right|\\right|_2^2\n",
    "$$\n",
    "\n",
    "Здесь $\\text{Gram}_j$ — это матрица Грама для активаций $j$-го слоя. Вообще, матрица Грама — это матрица, состоящая из скалярных произведений. Чтобы объяснить, какие скалярные произведения имеются в виду в этом случае, вспомним, что для каждого элемента батча активации $j$-го слоя имеют форму $C_j \\times H_j \\times W_j$. Теперь представим, что $f$ — это результат решейпа тензора активаций в форму $C_j \\times H_j \\cdot W_j$. Тогда матрица Грама — это такая матрица:\n",
    "\n",
    "```\n",
    "                        |----------|\n",
    "                        |   C_j    |\n",
    "    |---------------|   |          |   |----------|\n",
    "    |   W_j * H_j   |   |          |   |   C_j    |\n",
    "    |C_j            | x |W_j * H_j | = |C_j       |\n",
    "    |               |   |          |   |          |\n",
    "    |---------------|   |          |   |----------|\n",
    "                        |          |\n",
    "                        |----------|\n",
    "```\n",
    "\n",
    "То есть для каждого канала мы считаем скалярное произведение с каждым другим каналом по пространственным размерностям. Идея здесь в том, что это полностью уничтожает всю пространственную информацию.\n",
    "\n",
    "Мы будем использовать style loss на всех слоях, которые мы подготовили в нашем шаблоне `Vgg16`.\n",
    "\n",
    "Примечание: вполне возможно, что от такого количества коэффициентов можно избавиться, потюнив вес style loss в общем лоссе. Я позаимствовал эту схему из примера [`fast_neural_style`](https://github.com/pytorch/examples/tree/master/fast_neural_style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b991c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(t):\n",
    "    b, c, h, w = t.shape\n",
    "\n",
    "    # Реализуйте вычисление матрицы Грама.\n",
    "    # Это можно сделать многими способами. Возможно, вам понадобятся какие-то из этих функций:\n",
    "    # transpose, bmm, matmul, einsum\n",
    "\n",
    "    <YOUR CODE>\n",
    "    \n",
    "    return gram / (c * h * w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J_mNWEIXSLoP",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_loss(gen_gram_matrices, style_gram_matrices):\n",
    "    # {gen,style}_gram_matrices — это списки из 4 элементов, каждый из которых является матрицей Грама\n",
    "    <YOUR CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525335c",
   "metadata": {},
   "source": [
    "### Обучающий цикл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bada706",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(transformer.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1743579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(transformer, opt, style_tensor, content_tensor, dataloader, vgg, device, tb_dir,\n",
    "          epochs=2, content_weight=1e5, style_weight=1e10):\n",
    "    style_tensor = style_tensor.repeat(dataloader.batch_size, 1, 1, 1)  # to avoid a warning in F.mse_loss\n",
    "    style_tensor = style_tensor.to(device)\n",
    "\n",
    "    # Посчитайте список из матриц Грама для стилевой картинки\n",
    "    style_gram_matrices = <YOUR CODE>\n",
    "\n",
    "    content_tensor = content_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    batch_idx = 0\n",
    "    \n",
    "    with SummaryWriter(log_dir=str(tb_dir / get_datetime())) as writer:\n",
    "        for e in range(epochs):\n",
    "            for content_batch, _ in tqdm(dataloader):\n",
    "                content_batch = content_batch.to(device)\n",
    "\n",
    "                # Пропустите батч через генератор\n",
    "                gen_batch = <YOUR CODE>\n",
    "\n",
    "                # Посчитайте активации VGG на сгенерированных картинках\n",
    "                gen_features = <YOUR CODE>\n",
    "                content_features = <YOUR CODE>\n",
    "\n",
    "                # Посчитайте content loss\n",
    "                content_loss = <YOUR CODE>\n",
    "\n",
    "                # Посчитайте матрицы Грама по активациям сгенерированных картинок\n",
    "                gen_gram_matrices = [gram_matrix(f) for f in gen_features]\n",
    "\n",
    "                # Это нужно, чтобы избежать проблем на последнем батче\n",
    "                style_gram_matrices_truncated = [\n",
    "                    style_gram_matrix[:content_batch.shape[0]] for style_gram_matrix in style_gram_matrices\n",
    "                ]\n",
    "\n",
    "                # Посчитайте style loss (используя style_gram_matrices_truncated)\n",
    "                style_loss = <YOUR CODE>\n",
    "\n",
    "                # Посчитайте итоговый лосс с весами content_weight и style_weight\n",
    "                total_loss = <YOUR CODE>\n",
    "\n",
    "                # Сделайте шаг оптимизации\n",
    "                <YOUR CODE>\n",
    "                \n",
    "                writer.add_scalar('losses/content', content_loss.item(), batch_idx)\n",
    "                writer.add_scalar('losses/style', style_loss.item(), batch_idx)\n",
    "                writer.add_scalar('losses/total', total_loss.item(), batch_idx)\n",
    "\n",
    "                if (batch_idx + 1) % 100 == 0:\n",
    "                    transformer.eval()\n",
    "                    with torch.no_grad():\n",
    "                        y = transformer(content_tensor)\n",
    "                        writer.add_image('image', y.detach().squeeze(0), batch_idx)\n",
    "                    transformer.train()\n",
    "                \n",
    "                batch_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WYoiES94P0La",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_dir = Path('tb_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qW23t3EXPv3V",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --port 6007 --logdir $tb_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29bc687",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(transformer, opt, style_tensor, content_tensor, dataloader, vgg, device, tb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z1JC85pSlObo",
   "metadata": {},
   "source": [
    "На всякий случай ещё раз посмотрим, что наша модель выдаёт на валидационной картинке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52RzXsNvQUki",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.hstack([\n",
    "    torch_image_to_numpy(content_tensor),\n",
    "    torch_image_to_numpy(transformer(content_tensor.unsqueeze(0).to(device)).squeeze(0).detach().cpu()),\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k8twpKZZEKdz",
   "metadata": {},
   "source": [
    "А теперь погоняем нашу модель на разных картинках из интернета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7qkosKxvEIr3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stylize(url, factor=4):\n",
    "    image = get_image(url)\n",
    "\n",
    "    w, h = image.size\n",
    "    new_w = w // factor * factor\n",
    "    new_h = h // factor * factor\n",
    "\n",
    "    # Почему тут необходим CenterCrop? Почему factor именно 4?\n",
    "    transform_no_resize = transforms.Compose([\n",
    "        transforms.CenterCrop((new_h, new_w)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    image_tensor = transform_no_resize(image)\n",
    "    assert image_tensor.shape[1:] == (new_h, new_w)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        stylized_tensor = transformer(image_tensor.unsqueeze(0).to(device)).squeeze(0).cpu()\n",
    "\n",
    "    assert image_tensor.shape == stylized_tensor.shape\n",
    "\n",
    "    return Image.fromarray(\n",
    "        np.hstack([\n",
    "            torch_image_to_numpy(image_tensor),\n",
    "            torch_image_to_numpy(stylized_tensor),\n",
    "        ])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ZsKPeKVF5Ip",
   "metadata": {},
   "outputs": [],
   "source": [
    "stylize(content_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gv8JQDRoEv0y",
   "metadata": {},
   "outputs": [],
   "source": [
    "stylize('https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg/515px-Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_cSwWclSE60J",
   "metadata": {},
   "outputs": [],
   "source": [
    "stylize('https://data.whicdn.com/images/93462738/original.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ExVHpcB2lZJc",
   "metadata": {},
   "source": [
    "Кажется, что на больших картинках модель рисует очень мелкие детали витража — да и вообще эти детали имеют более-менее одинаковый размер в пикселях. А как бы сделать так, чтобы эти детали были разного размера?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
