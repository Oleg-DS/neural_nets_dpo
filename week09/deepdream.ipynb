{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "168b0228",
   "metadata": {},
   "source": [
    "# DeepDream\n",
    "\n",
    "Сегодня мы воспроизведём известную работу Александра Мордвинцева и соавторов, [заметку](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html) о которой они опубликовали в июне 2015 года.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/pw/ACtC-3emSpI1T1ILEk-DUKMXgtfyfdmsCOPTvPLyvUK-UoY0-iXOPGrxoLzlm_FbToUwaK-wCs_Mcgo7Yaodyd9spacJIR6xhrlMJYJX2XqIIYYxeYJ5h8-EDzCy5mb6a8eBTl0nZqdaqpY4LYtEPV1SjBF4=w716-h448-no)\n",
    "\n",
    "Этот ноутбук сделан на основе двух источников:\n",
    "\n",
    "* [Сторонняя реализация на PyTorch](https://github.com/eriklindernoren/PyTorch-Deep-Dream)\n",
    "* [Оригинальная реализация на Caffe](https://github.com/google/deepdream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbfb381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import scipy.ndimage as nd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630b705b",
   "metadata": {},
   "source": [
    "# 1. Загружаем картинку"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6947c0a",
   "metadata": {},
   "source": [
    "Экспериментировать будем на Мона Лизе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73960fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(io.BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992ba4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg/515px-Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a940ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "image = get_image(url)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5bfd04",
   "metadata": {},
   "source": [
    "# 2. Скачиваем предобученную модель"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f56a974",
   "metadata": {},
   "source": [
    "Общая идея — это сгенерировать картинку, которая будет вызывать сильную активацию одного из промежуточных слоёв в какой-то классификационной нейронке. Например, в одном из свёрточных слоёв VGG-19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f7734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15f13ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузите предобученную модель VGG-19\n",
    "network = <YOUR CODE>\n",
    "\n",
    "# Извлеките из неё свёрточную часть\n",
    "feature_extractor = <YOUR CODE>\n",
    "\n",
    "# Перенесите её на GPU\n",
    "<YOUR CODE>\n",
    "\n",
    "# Отключите подсчёт градиентов по её параметрам\n",
    "<YOUR CODE>\n",
    "\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ac1a77",
   "metadata": {},
   "source": [
    "В случае со свёрточной частью VGG-19 наша жизнь сильно упрощается тем, что это всего-навсего `nn.Sequential`, который имеет интерфейс, как у обычного питоновского списка. Это позволит нам очень легко добывать промежуточные активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd54fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db6beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f5a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(1, 3, 224, 224).to(device)\n",
    "activations = feature_extractor[:10](X)\n",
    "activations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac500e8",
   "metadata": {},
   "source": [
    "# 3. Реализуем алгоритм"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd5c0c",
   "metadata": {},
   "source": [
    "Базово DeepDream — это всего лишь максимизация суммы квадратов (т.е. квадрата $L_2$-нормы) активаций некоторого промежуточного слоя.\n",
    "\n",
    "$$\n",
    "\\mathbf{h} = \\operatorname{submodel}(\\mathbf{x}) \\\\\n",
    "\\sum_i h_i^2 \\to \\max_{\\mathbf{x}}\n",
    "$$\n",
    "\n",
    "Делать это мы будем градиентным спуском: будем в течение нескольких итераций считать градиент $\\sum_i h_i^2$ по $\\mathbf{x}$ и делать шаг **по** градиенту (не против, т.к. мы максимизируем это число).\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L &= \\sum_i h_i^2 \\\\\n",
    "g^{(t)} &= \\frac {\\partial L} {\\partial \\mathbf{x}^{(t)}} \\\\\n",
    "\\mathbf{x}^{(t + 1)} &= \\mathbf{x}^{(t)} + \\lambda^{(t)} g^{(t)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a4bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deepdream_grad(submodel, x):\n",
    "    submodel_input = x.detach()\n",
    "    submodel_input.requires_grad = True\n",
    "    \n",
    "    # Вычислите активации на выходе подмодели, посчитайте сумму их квадратов и верните градиент\n",
    "    <YOUR CODE>\n",
    "    \n",
    "    return submodel_input.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b23b42",
   "metadata": {},
   "source": [
    "Вокруг этой процедуры оптимизации в DeepDream применяется несколько костылей, чтобы картинки получались лучше. Будем реализовывать их по очереди.\n",
    "\n",
    "Во-первых, перед каждым вычислением и применением градиентов картинка параллельно сдвигается на случайный вектор, а после применения градиентов сдвигается обратно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73a8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_tensor(tensor, shift_h, shift_w):\n",
    "    return torch.roll(tensor, shifts=(shift_h, shift_w), dims=(1, 2))\n",
    "\n",
    "Image.fromarray(\n",
    "    shift_tensor(\n",
    "        torch.tensor(np.array(image)).permute(2, 0, 1),\n",
    "        shift_h=-100,\n",
    "        shift_w=50,\n",
    "    ).permute(1, 2, 0).numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de0932",
   "metadata": {},
   "source": [
    "Во-вторых, после каждого применения градиентов значения пикселей в тензоре обрезаются таким образом, чтобы не выходить за допустимые границы. Если бы мы работали с тензором в диапазоне `[0..1]` или `[0..255]`, то для такого обрезания было бы достаточно вызвать `torch.clamp(image, 0, 1)` (или, соответственно, `torch.clamp(image, 0, 255)`). Но мы работаем с изображениями в нормализации ImageNet, а поддержку границ-тензоров [добавили](https://github.com/pytorch/pytorch/pull/52695) в `torch.clamp` только 3 мая 2021 года, и на момент написания этого ноутбука эти изменения ещё не попали в релиз. Поэтому нам нужно реализовать самостоятельно разную нормировку для разных каналов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd184a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406])\n",
    "IMAGENET_STD = np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp(image_tensor):\n",
    "    \"\"\"\n",
    "    Clamp each channel in image_tensor (1x3xHxW torch.Tensor) to ImageNet normalization.\n",
    "    \"\"\"\n",
    "    # Посчитайте два np.array из трёх элементов каждый: минимальные и максимальные значения для тензоров,\n",
    "    # отнормированных при помощи IMAGENET_MEAN и IMAGENET_STD. Это значит, что до нормировки тензоры имели\n",
    "    # значения от 0 до 1 в каждом канале, но потом разные каналы отнормировали по-разному: из элементов\n",
    "    # канала `c` вычли IMAGENET_MEAN[c] и разделили на IMAGENET_STD[c].\n",
    "    lo = <YOUR CODE>\n",
    "    hi = <YOUR CODE>\n",
    "    for c in range(3):\n",
    "        image_tensor[:, c] = torch.clamp(image_tensor[:, c], lo[c], hi[c])\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668845c9",
   "metadata": {},
   "source": [
    "В-третьих, $\\lambda^{(t)}$ — не константа. Этот коэффициент вычисляется так:\n",
    "\n",
    "$$\n",
    "\\lambda^{(t)} = \\frac \\lambda { \\frac 1 n \\sum_{j = 1}^n |g^{(t)}_j| }\n",
    "$$\n",
    "\n",
    "Теперь напишем собственно цикл оптимизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f4d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dream(image, submodel, iterations, lr, jitter):\n",
    "    \"\"\" Updates the image to maximize outputs for n iterations \"\"\"\n",
    "    for _ in range(iterations):\n",
    "        # Сгенерируйте shift_h и shift_w от -jitter до jitter каждый и сдвиньте image на (shift_h, shift_w)\n",
    "        <YOUR CODE>\n",
    "        \n",
    "        # Посчитайте градиент\n",
    "        image_grad = <YOUR CODE>\n",
    "        \n",
    "        # Посчитайте отнормированный learning rate\n",
    "        <YOUR CODE>\n",
    "        \n",
    "        # Обновите image: сделайте шаг вдоль градиента и обрежьте значения пикселей\n",
    "        <YOUR CODE>\n",
    "        \n",
    "        # Не забудьте сдвинуть image обратно!\n",
    "        <YOUR CODE>\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2beb1c1",
   "metadata": {},
   "source": [
    "Давайте проверим, что у нас получается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "\n",
    "def torch_image_to_numpy(image_torch):\n",
    "    \"\"\"Convert PyTorch tensor to Numpy array.\n",
    "    :param image_torch: ImageNet-normalized PyTorch float CHW Tensor.\n",
    "    :returns: Numpy uint8 HWC array in range [0..255].\n",
    "    \"\"\"\n",
    "    image_np = image_torch.permute(1, 2, 0).numpy()\n",
    "    image_np = image_np * IMAGENET_STD + IMAGENET_MEAN\n",
    "    image_np = image_np * 255 + 0.5\n",
    "    image_np = np.clip(image_np, 0, 255)\n",
    "    image_np = image_np.astype(np.uint8)\n",
    "    return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d81104",
   "metadata": {},
   "outputs": [],
   "source": [
    "dreamed_image = torch_image_to_numpy(\n",
    "    dream(\n",
    "        transform_pipeline(image).unsqueeze(0).to(device),\n",
    "        feature_extractor[:28],\n",
    "        iterations=20,\n",
    "        lr=0.01,\n",
    "        jitter=32,\n",
    "    ).cpu().squeeze(dim=0)\n",
    ")\n",
    "\n",
    "Image.fromarray(dreamed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e581ed7e",
   "metadata": {},
   "source": [
    "Видны некоторые интересные эффекты, но как-то слабовато.\n",
    "\n",
    "Чтобы усилить эффект, в DeepDream применяется ещё один костыль: вычисления на разных масштабах. Из изображения делается целая серия изображений, где каждое следующее изображение больше предыдущего. В оригинальном коде такие изображения называются \"октавы\". Мы будем называть всю такую серию изображений \"пирамидой\", поскольку такое название более распространено."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373150fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def make_image_pyramid(image, scale_step, num_scales):\n",
    "    \"\"\"Returns a list of length `num_scales` where the *last* element is equal to `image`,\n",
    "    and each of the other elements is `scale_step` times smaller than the one after it.\"\"\"\n",
    "    \n",
    "    # Подсказка: здесь пригодится функция torch.nn.functional.interpolate. Используйте mode='bilinear'.\n",
    "    \n",
    "    <YOUR CODE>\n",
    "    \n",
    "    return pyramid\n",
    "\n",
    "pyramid = make_image_pyramid(transform_pipeline(image).unsqueeze(0).to(device), scale_step=2.5, num_scales=3)\n",
    "for item in pyramid:\n",
    "    display(Image.fromarray(torch_image_to_numpy(item.squeeze(dim=0).cpu())))\n",
    "del pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab7e49",
   "metadata": {},
   "source": [
    "Пирамиду мы используем для того, чтобы посчитать галлюцинации DeepDream на каждом из её масштабов, начиная с самого маленького."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d0b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def deep_dream(image, model, iterations=20, lr=0.01, scale_step=1.4, num_scales=10, jitter=32):\n",
    "    \"\"\" Main deep dream method \"\"\"\n",
    "    image = transform_pipeline(image).unsqueeze(dim=0).to(device)\n",
    "    \n",
    "    # Посчитайте пирамиду\n",
    "    pyramid = <YOUR CODE>\n",
    "\n",
    "    # Здесь мы будем поддерживать то, что уже посчитали на более мелких масштабах.\n",
    "    # Тут будут только артефакты DeepDream, без самого изображения.\n",
    "    deep_dream_detail = torch.zeros_like(pyramid[0])\n",
    "    \n",
    "    for scale_idx, scale in enumerate(tqdm(pyramid)):\n",
    "        if scale_idx > 0:\n",
    "            # Отмасштабируйте deep_dream_detail до размеров нового уровня пирамиды.\n",
    "            # Используйте torch.nn.functional.interpolate с mode='bilinear'.\n",
    "            <YOUR CODE>\n",
    "\n",
    "        # Добавьте посчитанные ранее артефакты к новому уровню пирамиды\n",
    "        input_image = <YOUR CODE>\n",
    "        \n",
    "        # Обновите картинку с артефактами\n",
    "        dreamed_image = <YOUR CODE>\n",
    "        \n",
    "        display(Image.fromarray(torch_image_to_numpy(dreamed_image.cpu().squeeze(dim=0))))\n",
    "        \n",
    "        # Уберите картинку из пирамиды из артефактов\n",
    "        <YOUR CODE>\n",
    "    \n",
    "    return torch_image_to_numpy(dreamed_image.cpu().squeeze(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0879e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dreamed_image = deep_dream(image, feature_extractor[:28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d562112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(dreamed_image)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
